{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import ollama\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoReply:\n",
    "    \n",
    "    def __init__(self,  user_id: str):\n",
    "        self.chroma_client = chromadb.Client()\n",
    "        \n",
    "        self.script = None\n",
    "        self.script_chunks = None\n",
    "        self.chromadb_collection = self.chroma_client.get_or_create_collection(name=user_id)\n",
    "        self.llm = ChatOllama(model='llama3', temperature=0, format='json')\n",
    "        self.prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            You are a helpful AI Assistant chatbot that replies to user queries from a livestream.\n",
    "            \n",
    "            Based on the following document:\n",
    "            {document}, and\n",
    "            \n",
    "            User Message: {user_message}\n",
    "\n",
    "            Determine if the User's Message is directly related to the document. If it is not relevant, return key value pair as Success: False, Message: None\n",
    "            \n",
    "            Else, if you are able to reply to the user's message given the document, return key value pair as Success: True, Message: <Your Response>\n",
    "            Adopt a helpful tone and be concise in your answers.\n",
    "            \"\"\"\n",
    "        )\n",
    "        self.chain = self.prompt | self.llm | JsonOutputParser()\n",
    "        \n",
    "        \n",
    "    # Read the script from an uploaded file\n",
    "    def _read_script(self, file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            script = file.read()\n",
    "        self.script = script\n",
    "        return script\n",
    "    \n",
    "    # Can look into other methods for splitting into chunks (this requires the user to split into paragraphs by 2 lines)\n",
    "    def _split_script(self, script, split_by='paragraph'):\n",
    "        if self.script is None:\n",
    "            raise ValueError(\"Script has not been added.\")\n",
    "        if split_by == 'paragraph':\n",
    "            # Split by double newline characters for paragraphs\n",
    "            chunks = script.split('\\n\\n')\n",
    "        elif split_by == 'sentence':\n",
    "            # Split by periods for sentences\n",
    "            chunks = script.split('. ')\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported split_by value. Use 'paragraph' or 'sentence'.\")\n",
    "        \n",
    "        self.script_chunks = chunks\n",
    "        return chunks\n",
    "    \n",
    "    def _get_embedding(self, text):\n",
    "        res = ollama.embeddings(model='nomic-embed-text', prompt=text)\n",
    "        return res['embedding']\n",
    "    \n",
    "    def _store_script_chunks(self, chunks):\n",
    "        embeddings = [self._get_embedding(chunk) for chunk in chunks]\n",
    "        documents = chunks\n",
    "        ids = list(str(x) for x in range(len(chunks)))\n",
    "        self.chromadb_collection.add(\n",
    "            documents=documents,\n",
    "            embeddings=embeddings,\n",
    "            ids=ids\n",
    "        )\n",
    "        \n",
    "    # Main function to process the script\n",
    "    def process_script(self, file_path):\n",
    "        script = self._read_script(file_path)\n",
    "        chunks = self._split_script(script, split_by='paragraph')\n",
    "        self._store_script_chunks(chunks)\n",
    "    \n",
    "    def check_collection(self):\n",
    "        return self.chromadb_collection.get()\n",
    "    \n",
    "    \n",
    "    # retrieve specific parts from script to reply user's message\n",
    "    def _get_relevant_document(self, message):\n",
    "        res = self.chromadb_collection.query(query_embeddings=[self._get_embedding(message)], n_results=1)\n",
    "        return res\n",
    "\n",
    "    # generate auto reply from retrieved document, if no retrieved documents, return None\n",
    "    def _get_auto_reply_json(self, message):\n",
    "        document = self._get_relevant_document(message)\n",
    "        if document:\n",
    "            reply = self.chain.invoke({\"document\": document, \"user_message\": message})\n",
    "            # Example of successful reply: {'Success': True, 'Message': 'The Slip-Ons come in versatile colors: grey, navy, and black.'}\n",
    "            # Example of unsuccesful reply: {'Success': False, 'Message': \"Hello! It seems like you're just saying hello. If you have any questions or need help with something specific, feel free to ask!\"}\n",
    "            return reply\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    # To use to determine if a user's message can be replied by the bot\n",
    "    def get_auto_response(self, message):\n",
    "        reply = self._get_auto_reply_json(message)\n",
    "        if reply['Success']:\n",
    "            return reply['Message']\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 2\n",
      "Insert of existing embedding ID: 3\n",
      "Insert of existing embedding ID: 4\n",
      "Insert of existing embedding ID: 5\n",
      "Insert of existing embedding ID: 6\n",
      "Insert of existing embedding ID: 7\n",
      "Add of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n",
      "Add of existing embedding ID: 4\n",
      "Add of existing embedding ID: 5\n",
      "Add of existing embedding ID: 6\n",
      "Add of existing embedding ID: 7\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "\n",
    "auto_reply = AutoReply(user_id=\"User0\")\n",
    "\n",
    "# Uploading of user's script\n",
    "file_path = \"../data/scripts/example_script\" \n",
    "auto_reply.process_script(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The slip-ons are available in grey, navy, and black.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should return a reply since the answer can be found from the scirpt\n",
    "auto_reply.get_auto_response(message=\"What colour are the slip ons available in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should return None\n",
    "auto_reply.get_auto_response(message=\"Hello!!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
